{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Word Embeddings \n",
    "        Its used for the representation of words for text analysis, typically in the form of a real-valued vector \n",
    "that encodes the meaning of the workd such that the words that are closer in the vector space are expected to be similar in meaning.\n",
    "\n",
    "\n",
    " 2 types : \n",
    " 1. count or frequency - OHE, BOW, TFIDF \n",
    " 2. Deep learning based - WORD2VEC\n",
    "\n",
    " WORD2VEC 2 types : \n",
    " 1. CBOW - Continuous Bow - uses ANN\n",
    " 2. Skipgram \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#### WORD2VEC --> \"Feature representation\"\n",
    "1. Uses a neural nw to learn word associations from a large corpus of text.\n",
    "2. Once trained model can detect --> synonymous words or suggest additional words for a partial sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ğŸ§  Word2Vec â€“ CBOW Model (Complete Notes)\n",
    "ğŸ“˜ 1. What is Word2Vec?\n",
    "\n",
    "Word2Vec is a technique to represent words as vectors (numbers) so that similar words have similar embeddings (like â€œkingâ€ and â€œqueenâ€).\n",
    "\n",
    "Instead of treating words as separate symbols, Word2Vec captures their meaning based on how they appear with other words in a sentence.\n",
    "\n",
    "âš™ï¸ 2. Two types of Word2Vec models\n",
    "Model\tPredicts\tDescription\n",
    "CBOW (Continuous Bag of Words)\tPredicts the target word from its context\tFast for small data\n",
    "Skip-Gram\tPredicts context words from the target word\tBetter for rare words\n",
    "\n",
    "Weâ€™ll focus on CBOW here.\n",
    "\n",
    "ğŸ§© 3. CBOW Concept (Intuition)\n",
    "\n",
    "CBOW learns by guessing the missing word in the middle of a sentence using the words around it.\n",
    "\n",
    "Example:\n",
    "\n",
    "Sentence:\n",
    "\n",
    "â€œThe cat drinks milkâ€\n",
    "\n",
    "If window size = 1 â†’ context = 1 word before and after\n",
    "\n",
    "Training examples:\n",
    "\n",
    "Context Words\tTarget Word\n",
    "[The, drinks]\tcat\n",
    "[cat, milk]\tdrinks\n",
    "[drinks]\tmilk\n",
    "ğŸ§± 4. Architecture Overview\n",
    "CBOW has 3 layers:\n",
    "Layer\tDescription\n",
    "Input layer\tTakes the context words as input\n",
    "Hidden layer\tLearns the embedding (vector representation)\n",
    "Output layer\tPredicts the target word\n",
    "ğŸ§  5. Step-by-Step Flow\n",
    "\n",
    "Letâ€™s use a small example:\n",
    "\n",
    "Vocabulary (after preprocessing):\n",
    "[\"cat\", \"drinks\", \"milk\", \"dog\", \"eats\", \"bone\"]\n",
    "\n",
    "\n",
    "â†’ Vocabulary size = 6\n",
    "â†’ Embedding size = 3 (for simplicity)\n",
    "\n",
    "ğŸ”¹ Step 1: Input Layer\n",
    "\n",
    "Each unique word is represented as a one-hot vector (6 elements).\n",
    "\n",
    "Only the context words are â€œactivatedâ€ (set to 1).\n",
    "\n",
    "For context [\"cat\", \"milk\"], input is:\n",
    "\n",
    "Word\tOne-hot vector\n",
    "cat\t[1, 0, 0, 0, 0, 0]\n",
    "milk\t[0, 0, 1, 0, 0, 0]\n",
    "\n",
    "Both these go into the model.\n",
    "\n",
    "ğŸ”¹ Step 2: Hidden Layer\n",
    "\n",
    "Weight matrix W1 (size = 6Ã—3) stores the embeddings for all words.\n",
    "\n",
    "For each active word (cat and milk), we pick its row vector from W1.\n",
    "\n",
    "Then we average the two vectors.\n",
    "\n",
    "Example:\n",
    "\n",
    "cat â†’ [0.8, 0.3, 0.6]\n",
    "milk â†’ [0.4, 0.7, 0.1]\n",
    "Average = [(0.8+0.4)/2, (0.3+0.7)/2, (0.6+0.1)/2]\n",
    "        = [0.6, 0.5, 0.35]\n",
    "\n",
    "\n",
    "This [0.6, 0.5, 0.35] is the hidden layer output (the context embedding).\n",
    "\n",
    "ğŸ”¹ Step 3: Output Layer\n",
    "\n",
    "Weight matrix W2 (size = 3Ã—6) maps hidden layer â†’ output layer.\n",
    "\n",
    "Model predicts probabilities for each of the 6 words.\n",
    "\n",
    "The word with the highest probability is the target word.\n",
    "\n",
    "For our example, the model tries to predict:\n",
    "\n",
    "Target = â€œdrinksâ€\n",
    "\n",
    "ğŸ”¹ Step 4: Learning\n",
    "\n",
    "During training:\n",
    "\n",
    "The model compares predicted vs actual target.\n",
    "\n",
    "It adjusts weights (W1 and W2) using backpropagation.\n",
    "\n",
    "Over many sentences, words that appear in similar contexts end up with similar embeddings.\n",
    "\n",
    "Example outcome:\n",
    "\n",
    "king â€“ man + woman â‰ˆ queen\n",
    "cat â€“ animal + milk â‰ˆ drink\n",
    "\n",
    "âš–ï¸ 6. Important Details\n",
    "Concept\tMeaning\n",
    "Input layer size\tNumber of unique words in vocabulary\n",
    "Output layer size\tSame as input (same vocabulary)\n",
    "Hidden layer size\tEmbedding dimension (e.g., 100 or 300)\n",
    "W1\tInput â†’ hidden (stores word embeddings) (features)\n",
    "W2\tHidden â†’ output (used for prediction)\n",
    "Same vocabulary\tâœ… Yes, both input and output layers use the same set of words\n",
    "Same weights\tâŒ No, input (W1) and output (W2) are different matrices\n",
    "ğŸ§® 7. Mathematical Summary\n",
    "\n",
    "Let:\n",
    "\n",
    "Vocabulary size = V\n",
    "\n",
    "Embedding size = N\n",
    "\n",
    "Context size = C\n",
    "\n",
    "Then:\n",
    "\n",
    "Input layer: V Ã— 1 (one-hot for each context word)\n",
    "Hidden layer: N Ã— 1 (averaged embedding)\n",
    "Output layer: V Ã— 1 (softmax probabilities)\n",
    "\n",
    "\n",
    "Weights:\n",
    "\n",
    "W1 = V Ã— N  â†’ input â†’ hidden\n",
    "W2 = N Ã— V  â†’ hidden â†’ output\n",
    "\n",
    "\n",
    "Computation for one training example (with C context words):\n",
    "\n",
    "1ï¸âƒ£ Get embeddings for all context words from W1\n",
    "2ï¸âƒ£ Take their average â†’ hidden vector h\n",
    "3ï¸âƒ£ Multiply by W2 â†’ get output scores\n",
    "4ï¸âƒ£ Apply softmax â†’ probabilities\n",
    "5ï¸âƒ£ Update W1 and W2 using error (target word)\n",
    "\n",
    "ğŸ¯ 8. Intuitive Summary\n",
    "Step\tWhat happens\tAnalogy\n",
    "Input\tContext words come in\tStudents give clues\n",
    "Hidden\tEmbeddings averaged\tTeacher summarizes clues\n",
    "Output\tPredicts target word\tTeacher guesses the missing word\n",
    "Training\tAdjusts weights\tTeacher learns better clues over time\n",
    "ğŸ’¬ 9. Common Doubts\n",
    "Question\tAnswer\n",
    "Are input and output layers same?\tSame vocabulary, different roles\n",
    "Are all words passed at once?\tNo, one target at a time (based on window)\n",
    "Each sentence has separate layers?\tâŒ No, all sentences share the same model (weights)\n",
    "When do we combine words?\tIn hidden layer â€” we average context word embeddings\n",
    "What does hidden layer store?\tWord meanings (dense features)\n",
    "ğŸ” 10. Final Summary in One Line\n",
    "\n",
    "CBOW learns word meanings by predicting the center word from the average of its neighboring words,\n",
    "using the same vocabulary for input and output, but different roles and weights.\n",
    "\n",
    "ğŸ§¾ 11. Quick Formula Recap\n",
    "h = (1/C) * Î£ (W1áµ€ * x_c)\n",
    "u = W2áµ€ * h\n",
    "y_pred = softmax(u)\n",
    "Loss = -log(y_pred[target])\n",
    "\n",
    "\n",
    "Then update W1, W2 to reduce loss.\n",
    "\n",
    "ğŸ’¡ Tip for Remembering:\n",
    "\n",
    "CBOW = â€œPredict the center word from the bag of surrounding words.â€\n",
    "Skip-Gram = â€œPredict the surrounding words from the center word.â€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ğŸ§  Word2Vec â€“ Skip-Gram Model (Complete Notes)\n",
    "ğŸ“˜ 1. What is Skip-Gram?\n",
    "\n",
    "Skip-Gram is the second version of Word2Vec.\n",
    "\n",
    "Itâ€™s the opposite of CBOW:\n",
    "\n",
    "CBOW â†’ predicts target word from context\n",
    "\n",
    "Skip-Gram â†’ predicts context words from target word\n",
    "\n",
    "Think of it as:\n",
    "\n",
    "â€œIf I know one word, can I guess which words usually come around it?â€\n",
    "\n",
    "ğŸ§© 2. Example for Intuition\n",
    "\n",
    "Sentence:\n",
    "\n",
    "â€œThe cat drinks milk dailyâ€\n",
    "\n",
    "Letâ€™s use window size = 2 (2 words before and after the target).\n",
    "\n",
    "ğŸ”¹ CBOW vs Skip-Gram\n",
    "Model\tInput\tOutput\n",
    "CBOW\tContext words\tTarget word\n",
    "Skip-Gram\tTarget word\tContext words\n",
    "ğŸ§® 3. Skip-Gram Example (Training Samples)\n",
    "\n",
    "With window = 2, for each target word we create multiple training pairs:\n",
    "\n",
    "Sentence: â€œcat drinks milk dailyâ€\n",
    "Target (input)\tContext (outputs)\n",
    "cat\tâ†’ â€œTheâ€, â€œdrinksâ€\n",
    "drinks\tâ†’ â€œcatâ€, â€œmilkâ€, â€œdailyâ€\n",
    "milk\tâ†’ â€œdrinksâ€, â€œdailyâ€\n",
    "daily\tâ†’ â€œmilkâ€\n",
    "\n",
    "So each pair becomes one training example.\n",
    "\n",
    "Example pairs:\n",
    "\n",
    "(cat â†’ The)\n",
    "(cat â†’ drinks)\n",
    "(drinks â†’ cat)\n",
    "(drinks â†’ milk)\n",
    "(drinks â†’ daily)\n",
    "...\n",
    "\n",
    "ğŸ§± 4. Architecture Overview\n",
    "Layer\tDescription\n",
    "Input layer\tTakes one word (the target)\n",
    "Hidden layer\tEmbedding of that word\n",
    "Output layer\tPredicts multiple context words\n",
    "ğŸ§  5. Step-by-Step Flow\n",
    "\n",
    "Letâ€™s use the same vocabulary:\n",
    "\n",
    "[\"cat\", \"drinks\", \"milk\", \"dog\", \"eats\", \"bone\"]\n",
    "\n",
    "\n",
    "â†’ Vocabulary size = 6\n",
    "â†’ Embedding size = 3\n",
    "\n",
    "ğŸ”¹ Step 1: Input Layer\n",
    "\n",
    "Input is one word (the target).\n",
    "Example: target = â€œdrinksâ€\n",
    "\n",
    "One-hot vector of â€œdrinksâ€:\n",
    "\n",
    "[0, 1, 0, 0, 0, 0]\n",
    "\n",
    "\n",
    "(one hot, 1 at index of â€œdrinksâ€)\n",
    "\n",
    "ğŸ”¹ Step 2: Hidden Layer\n",
    "\n",
    "Weight matrix W1 (6Ã—3) stores the embeddings.\n",
    "\n",
    "Since only one input word is active (â€œdrinksâ€),\n",
    "we just pick its embedding row from W1.\n",
    "\n",
    "Example:\n",
    "\n",
    "â€œdrinksâ€ â†’ [0.6, 0.3, 0.9]\n",
    "\n",
    "\n",
    "This 3-dim vector becomes the hidden layer output.\n",
    "\n",
    "ğŸ”¹ Step 3: Output Layer\n",
    "\n",
    "Weight matrix W2 (3Ã—6) maps hidden layer â†’ output layer.\n",
    "\n",
    "Output layer predicts probabilities for all 6 words in vocabulary.\n",
    "\n",
    "Model should assign high probability to the real context words around â€œdrinksâ€ (like â€œcatâ€, â€œmilkâ€, â€œdailyâ€)\n",
    "and low probability to unrelated words (â€œboneâ€, â€œdogâ€, etc.).\n",
    "\n",
    "ğŸ”¹ Step 4: Learning\n",
    "\n",
    "The model compares predicted vs. actual context words.\n",
    "\n",
    "Updates W1 and W2 using backpropagation.\n",
    "\n",
    "Over time, it learns that words appearing in similar contexts have similar embeddings.\n",
    "\n",
    "âš–ï¸ 6. Key Differences between CBOW and Skip-Gram\n",
    "Feature\tCBOW\tSkip-Gram\n",
    "Input\tContext words\tTarget word\n",
    "Output\tTarget word\tContext words\n",
    "Speed\tFaster\tSlower\n",
    "Works better for\tFrequent words\tRare words\n",
    "Training samples\tFewer (one per target)\tMore (multiple per target)\n",
    "Mathematical operation\tAverage of input embeddings\tReuse one input embedding multiple times\n",
    "ğŸ§® 7. Mathematical Summary\n",
    "\n",
    "Let:\n",
    "\n",
    "Vocabulary size = V\n",
    "\n",
    "Embedding size = N\n",
    "\n",
    "Context size = C\n",
    "\n",
    "Then:\n",
    "\n",
    "Input layer: V Ã— 1 (one-hot for target word)\n",
    "Hidden layer: N Ã— 1 (embedding for target)\n",
    "Output layer: V Ã— 1 (softmax probabilities)\n",
    "\n",
    "\n",
    "Weights:\n",
    "\n",
    "W1 = V Ã— N  â†’ input â†’ hidden\n",
    "W2 = N Ã— V  â†’ hidden â†’ output\n",
    "\n",
    "\n",
    "Computation:\n",
    "\n",
    "1ï¸âƒ£ Input x (one-hot for target)\n",
    "2ï¸âƒ£ Hidden layer h = W1áµ€ Ã— x â†’ embedding of target word\n",
    "3ï¸âƒ£ Output layer u = W2áµ€ Ã— h\n",
    "4ï¸âƒ£ Apply softmax to get probability for each word\n",
    "5ï¸âƒ£ Update weights to make true context words more likely.\n",
    "\n",
    "ğŸ§¾ 8. Intuitive Analogy\n",
    "\n",
    "Think of Skip-Gram like a detective ğŸ•µï¸â€â™‚ï¸\n",
    "\n",
    "You show the detective one word (â€œdrinksâ€).\n",
    "\n",
    "He tries to guess which other words usually appear near it (â€œcatâ€, â€œmilkâ€, â€œdailyâ€).\n",
    "\n",
    "If he guesses wrong, he adjusts his thinking (updates weights).\n",
    "\n",
    "Over time, he learns strong connections like:\n",
    "\n",
    "drinks â†’ milk\n",
    "dog â†’ bone\n",
    "cat â†’ milk\n",
    "\n",
    "ğŸ’¡ 9. Example Flow\n",
    "\n",
    "Target word = â€œdrinksâ€\n",
    "Context words = [â€œcatâ€, â€œmilkâ€]\n",
    "\n",
    "Training pairs:\n",
    "\n",
    "(drinks â†’ cat)\n",
    "(drinks â†’ milk)\n",
    "\n",
    "\n",
    "For each:\n",
    "\n",
    "Input one-hot(â€œdrinksâ€)\n",
    "\n",
    "Get its embedding\n",
    "\n",
    "Predict which word in vocab matches â€œcatâ€ (or â€œmilkâ€)\n",
    "\n",
    "Update weights to improve prediction\n",
    "\n",
    "âœ… 10. Summary Table\n",
    "Layer\tDescription\n",
    "Input layer\tOne-hot of target word\n",
    "Hidden layer\tEmbedding of target\n",
    "Output layer\tPredicts probabilities for all words\n",
    "Context words\tTrue outputs\n",
    "W1\tStores embeddings\n",
    "W2\tOutput weights\n",
    "Loss\tAverage over all context predictions\n",
    "ğŸ§­ 11. Comparison Summary (CBOW vs Skip-Gram)\n",
    "Feature\tCBOW\tSkip-Gram\n",
    "Input\tMultiple context words\tSingle target word\n",
    "Output\tOne target word\tMultiple context words\n",
    "Computation per step\tAverage embeddings\tMultiple predictions\n",
    "Best for\tFrequent words\tRare words\n",
    "Idea\tâ€œGuess center word from neighborsâ€\tâ€œGuess neighbors from center wordâ€\n",
    "ğŸ§® 12. Mathematical Formula\n",
    "\n",
    "For one target word w_t and context words {w_tâˆ’C, ..., w_t+C}:\n",
    "\n",
    "Loss = average of all context word losses:\n",
    "\n",
    "L = - Î£ log P(w_context | w_target)\n",
    "\n",
    "\n",
    "Where\n",
    "\n",
    "P(w_context | w_target) = softmax(W2áµ€ * W1[w_target])\n",
    "\n",
    "ğŸ¯ 13. Intuitive Summary\n",
    "\n",
    "Skip-Gram learns word meaning by trying to predict neighboring words for each target word.\n",
    "Words that occur in similar contexts get similar vector representations.\n",
    "\n",
    "ğŸ§® 14. Visualization of Flow\n",
    "Input word (one-hot)\n",
    "       â†“\n",
    "Embedding lookup (W1)\n",
    "       â†“\n",
    "Hidden layer (vector of target word)\n",
    "       â†“\n",
    "Output layer (softmax for all vocab words)\n",
    "       â†“\n",
    "Predict context words\n",
    "\n",
    "ğŸ§  15. Simple Analogy with Example\n",
    "\n",
    "Sentence: â€œdog eats boneâ€\n",
    "\n",
    "Training pair\tMeaning\n",
    "(dog â†’ eats)\tdog often appears before eats\n",
    "(eats â†’ dog)\teats happens after dog\n",
    "(eats â†’ bone)\teats and bone are related\n",
    "(bone â†’ eats)\tbone appears with eats\n",
    "\n",
    "â†’ So the model learns â€œdogâ€, â€œeatsâ€, â€œboneâ€ are connected.\n",
    "\n",
    "ğŸ§¾ 16. Key Takeaways\n",
    "Concept\tDescription\n",
    "Skip-Gram direction\tTarget â†’ Context\n",
    "Embeddings learned from\tInput layer weights (W1)\n",
    "Same vocabulary in input & output\tâœ… Yes\n",
    "Same weights?\tâŒ No, W1 â‰  W2\n",
    "Whatâ€™s learned finally?\tWord embeddings (meaningful word vectors)\n",
    "Why â€œSkip-Gramâ€?\tBecause we skip the center word to predict its neighbors\n",
    "ğŸ§© 17. Quick Formula Recap\n",
    "Input: x (one-hot target)\n",
    "Hidden: h = W1áµ€ Ã— x\n",
    "Output: y_pred = softmax(W2áµ€ Ã— h)\n",
    "Loss = - Î£ log(y_pred[context])\n",
    "\n",
    "ğŸ’¬ 18. One-Line Summary\n",
    "\n",
    "Skip-Gram: Predicts context words given a center (target) word.\n",
    "CBOW and Skip-Gram are mirror images â€” one predicts the center, the other predicts the surroundings.\n",
    "\n",
    "\n",
    "ğŸ§  RECAP from CBOW (to connect your thinking)\n",
    "\n",
    "In CBOW:\n",
    "\n",
    "Input â†’ multiple context words\n",
    "\n",
    "Output â†’ single target word\n",
    "\n",
    "And yes â€”\n",
    "W1 learns how each word relates to hidden features (like â€œroyaltyâ€, â€œgenderâ€, â€œanimalâ€, â€œfoodâ€, etc.)\n",
    "so every word gets a unique feature-based vector (its embedding).\n",
    "\n",
    "ğŸ”„ Now in Skip-Gram\n",
    "\n",
    "In Skip-Gram, the direction changes â€”\n",
    "but the learning idea for W1 is the same!\n",
    "It still learns how each word connects to features.\n",
    "\n",
    "Letâ€™s see how ğŸ‘‡\n",
    "\n",
    "ğŸ§© Step 1: Input â†’ One Word (Target)\n",
    "\n",
    "Example:\n",
    "Sentence = â€œcat drinks milk dailyâ€\n",
    "Target = â€œdrinksâ€\n",
    "Context = [â€œcatâ€, â€œmilkâ€, â€œdailyâ€]\n",
    "\n",
    "Input layer = one-hot of â€œdrinksâ€\n",
    "\n",
    "ğŸ§© Step 2: W1 â€” Input â†’ Hidden\n",
    "\n",
    "W1 matrix = (vocab_size Ã— embedding_dim)\n",
    "\n",
    "Each row = embedding of one word\n",
    "\n",
    "When input = â€œdrinksâ€,\n",
    "we pick the â€œdrinksâ€ row from W1.\n",
    "\n",
    "So if embedding dimension = 3,\n",
    "then:\n",
    "\n",
    "W1[\"drinks\"] = [0.7, 0.2, 0.9]\n",
    "\n",
    "\n",
    "â†’ this means â€œdrinksâ€ is represented using 3 features.\n",
    "\n",
    "Each value shows how strongly the word â€œdrinksâ€ connects to each hidden feature.\n",
    "\n",
    "âœ… So just like in CBOW â€” W1 still tells us word-to-feature relationship.\n",
    "\n",
    "ğŸ§© Step 3: Hidden â†’ Output (W2)\n",
    "\n",
    "Now, using W2, the model predicts context words:\n",
    "â€œcatâ€, â€œmilkâ€, â€œdailyâ€.\n",
    "\n",
    "If the prediction is wrong,\n",
    "the error flows backward, updating both:\n",
    "\n",
    "W2 (for prediction strength)\n",
    "\n",
    "W1 (for better feature understanding)\n",
    "\n",
    "So â€œdrinksâ€ embedding (in W1) adjusts slightly\n",
    "to become more compatible with â€œcatâ€, â€œmilkâ€, and â€œdailyâ€.\n",
    "\n",
    "ğŸ§  Step 4: Whatâ€™s Actually Learned\n",
    "\n",
    "After training on many sentences:\n",
    "\n",
    "â€œdrinksâ€ will move close (in feature space) to â€œmilkâ€, â€œwaterâ€, â€œeatâ€, â€œjuiceâ€ â€” all related words.\n",
    "\n",
    "â€œcatâ€ will move close to â€œdogâ€, â€œpetâ€, â€œanimalâ€, etc.\n",
    "\n",
    "So W1 ends up storing meaningful word embeddings,\n",
    "where each wordâ€™s vector shows which hidden features it relates to.\n",
    "\n",
    "âš™ï¸ Comparison Summary\n",
    "Concept\tCBOW\tSkip-Gram\n",
    "W1 meaning\tEach word â†’ feature vector\tEach word â†’ feature vector\n",
    "Direction\tContext â†’ Target\tTarget â†’ Context\n",
    "What W1 learns\tHow each word connects to features (semantics)\tSame â€” how each word connects to features (semantics)\n",
    "Whatâ€™s different\tHow the model uses those embeddings (averaging vs predicting multiple)\tUses the same W1 idea but reversed logic\n",
    "ğŸ§® Intuitive Example\n",
    "\n",
    "Imagine these 3 hidden â€œfeaturesâ€:\n",
    "\n",
    "Feature 1\tFeature 2\tFeature 3\n",
    "â€œanimalâ€\tâ€œfood/drinkâ€\tâ€œdaily lifeâ€\n",
    "\n",
    "Now,\n",
    "\n",
    "â€œcatâ€ â†’ [0.9, 0.1, 0.2] â†’ strongly related to animal\n",
    "\n",
    "â€œdrinksâ€ â†’ [0.2, 0.8, 0.6] â†’ strongly related to food/drink\n",
    "\n",
    "â€œmilkâ€ â†’ [0.3, 0.9, 0.4] â†’ strongly related to food/drink\n",
    "\n",
    "â€œdogâ€ â†’ [0.8, 0.2, 0.1] â†’ strongly related to animal\n",
    "\n",
    "So in Skip-Gram, W1 still learns exactly this â€”\n",
    "how each word aligns with those internal â€œmeaningâ€ features.\n",
    "\n",
    "âœ… Final Summary\n",
    "Question\tAnswer\n",
    "Does W1 learn â€œword â†” featureâ€ relationships?\tâœ… Yes, in both CBOW and Skip-Gram\n",
    "Whatâ€™s different between them?\tOnly how W1 gets updated (direction of prediction)\n",
    "Whatâ€™s stored finally in W1?\tThe word embeddings â€” each word represented by feature values\n",
    "Whatâ€™s W2â€™s job?\tHelps predict other words (context); not used as final embedding\n",
    "Which matrix do we save for embeddings?\tâœ… W1 (the input â†’ hidden weights)\n",
    "\n",
    "ğŸ§  In simple words:\n",
    "\n",
    "In both CBOW and Skip-Gram,\n",
    "W1 is like a dictionary of meanings.\n",
    "Each word learns how much it belongs to different hidden features.\n",
    "The only difference is:\n",
    "\n",
    "CBOW learns it by averaging context to predict the center.\n",
    "\n",
    "Skip-Gram learns it by predicting contexts from the center."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
